import dotenv from "dotenv"
import { Kafka } from "kafkajs"
import { Pool } from "pg"
import { S3Client, GetObjectCommand } from "@aws-sdk/client-s3"
import pdfParse from "pdf-parse"

dotenv.config()

// ğŸ”¹ Database
const db = new Pool({
  connectionString: process.env.DATABASE_URL
})

// ğŸ”¹ Kafka
const kafka = new Kafka({
  clientId: "ingestion-service",
  brokers: ["localhost:9092"]
})

const consumer = kafka.consumer({ groupId: "document-ingestion-group" })
const producer = kafka.producer()

// ğŸ”¹ S3
const s3 = new S3Client({
  region: process.env.AWS_REGION,
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!
  }
})

// ğŸ”¹ Helper: Convert stream to buffer
async function streamToBuffer(stream: any): Promise<Buffer> {
  return new Promise((resolve, reject) => {
    const chunks: Buffer[] = []
    stream.on("data", (chunk: Buffer) => chunks.push(chunk))
    stream.on("error", reject)
    stream.on("end", () => resolve(Buffer.concat(chunks)))
  })
}

async function start() {
  await consumer.connect()
  await producer.connect()

  await consumer.subscribe({
    topic: "document.uploaded",
    fromBeginning: false
  })

  console.log("ğŸš€ Ingestion Service started...")

  await consumer.run({
    eachMessage: async ({ message }) => {
      if (!message.value) return

      const payload = JSON.parse(message.value.toString())
      const { documentId, storageKey } = payload

      console.log("ğŸ“¥ Received event:", payload)

      const maxRetries = 3
      let attempt = 0

      try {
        // ğŸ”¹ Set status to processing once
        await db.query(
          `UPDATE documents SET status = 'processing' WHERE id = $1`,
          [documentId]
        )
      } catch (err) {
        console.error("âŒ Failed to mark document as processing:", err)
        return
      }

      while (attempt < maxRetries) {
        try {
          console.log(`ğŸ”„ Processing ${documentId}, attempt ${attempt + 1}`)

          // ğŸ”½ Download file from S3
          const command = new GetObjectCommand({
            Bucket: process.env.AWS_BUCKET!,
            Key: storageKey
          })

          const response = await s3.send(command)

          if (!response.Body) {
            throw new Error("S3 returned empty body")
          }

          const fileBuffer = await streamToBuffer(response.Body)

          // ğŸ”½ Extract PDF text
          const pdfData = await pdfParse(fileBuffer)

          console.log(
            `ğŸ“„ Extracted text length: ${pdfData.text.length}`
          )

          // TODO: Chunk + embeddings here later

          // ğŸ”¹ Mark as ready
          await db.query(
            `UPDATE documents SET status = 'ready' WHERE id = $1`,
            [documentId]
          )

          console.log("âœ… Document processed successfully:", documentId)
          return

        } catch (err) {
          attempt++
          console.error(
            `âš ï¸ Attempt ${attempt} failed for ${documentId}:`,
            err
          )

          if (attempt < maxRetries) {
            const backoff = Math.pow(2, attempt) * 1000
            console.log(`â³ Retrying in ${backoff}ms...`)
            await new Promise(res => setTimeout(res, backoff))
          }
        }
      }

      // ğŸš¨ All retries exhausted â†’ Send to DLQ
      console.error(
        `ğŸš¨ Max retries exceeded for ${documentId}. Sending to DLQ.`
      )
      console.error("ğŸ“¦ DLQ Payload:", payload)

      try {
        await producer.send({
          topic: "document.uploaded.dlq",
          messages: [
            { value: JSON.stringify(payload) }
          ]
        })

        await db.query(
          `UPDATE documents SET status = 'failed' WHERE id = $1`,
          [documentId]
        )

        console.log("ğŸ›‘ Document marked as failed:", documentId)
      } catch (dlqError) {
        console.error("âŒ Failed to send to DLQ:", dlqError)
      }
    }
  })
}

start().catch(err => {
  console.error("ğŸ’¥ Ingestion service crashed:", err)
})
